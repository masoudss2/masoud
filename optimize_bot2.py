import os
import logging
import signal
import sys
import time
from functools import lru_cache
from io import BytesIO
from typing import Optional, Dict, Any, List
import json
from datetime import datetime, timedelta
import random  # Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes
)
import requests
from PyPDF2 import PdfReader
import tabula
from cryptography.fernet import Fernet
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from cachetools import TTLCache, cached
import traceback


# --- Configuration ---
# ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ØªÙ‚ÛŒÙ… Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ .env
TELEGRAM_TOKEN = "7429551898:AAF0BnBcQwNmi7IRA3PPVNf-K-4On2JROgs"  # ØªÙˆÚ©Ù† ØªÙ„Ú¯Ø±Ø§Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
DEEPSEEK_API_KEY = "sk-033cc340ba3247f7931a64c5e3d77330"  # Ú©Ù„ÛŒØ¯ API Ø¯ÛŒÙ¾â€ŒØ³ÛŒÚ© Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
ALPHA_VANTAGE_API_KEY = "8RD7DN1R2W5AI9UT"  # Ú©Ù„ÛŒØ¯ API Ø¢Ù„ÙØ§ ÙˆÙ†ØªÛŒØ¬ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
FINANCIAL_MODELING_PREP_API_KEY = "jBxtfLbURIAQQnzoQlL1ywKM72hrbAZT"  # Ú©Ù„ÛŒØ¯ API ÙØ§ÛŒÙ†Ù†Ø´Ø§Ù„ Ù…Ø§Ø¯Ù„ÛŒÙ†Ú¯ Ù¾Ø±Ù¾ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯

# ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ
ENCRYPTION_KEY = Fernet.generate_key()

# Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ API
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
ALPHA_VANTAGE_BASE_URL = "https://www.alphavantage.co/query"
FMP_BASE_URL = "https://financialmodelingprep.com/api/v3"

CACHE_SIZE = 100  # Maximum cached responses
CACHE_TTL = 259200 # Time to live for cache items (30 days)
MAX_PDF_PAGES = 10  # Prevent processing large PDFs
MAX_TEXT_LENGTH = 3000  # Character limit for API inputs
# Initialize caches with TTL
news_cache = TTLCache(maxsize=50, ttl=CACHE_TTL)
stock_cache = TTLCache(maxsize=50, ttl=CACHE_TTL)
market_cache = TTLCache(maxsize=10, ttl=CACHE_TTL/2)  # Market data expires faster

# Initialize Fernet for encryption
fernet = Fernet(ENCRYPTION_KEY)

# --- Logging --
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
    handlers=[
        logging.FileHandler("bot.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- Utility Functions ---
def encrypt_data(data: str) -> str:
    """Encrypt sensitive data before storage"""
    return fernet.encrypt(data.encode()).decode()

def decrypt_data(encrypted_data: str) -> str:
    """Decrypt stored data"""
    return fernet.decrypt(encrypted_data.encode()).decode()

def normalize_prompt(prompt: str) -> str:
    """Standardize prompts for effective caching"""
    return prompt.strip().replace("\n", " ").replace("  ", " ")

# --- PDF Processing ---
def process_pdf(pdf_bytes: BytesIO) -> dict:
    """Extract text and tables from PDF with error handling"""
    try:
        # Text extraction
        reader = PdfReader(pdf_bytes)
        text = " ".join([page.extract_text() or "" for page in reader.pages[:MAX_PDF_PAGES]])
        
        # Table extraction
        tables = tabula.read_pdf(
            pdf_bytes, 
            pages=f"1-{min(MAX_PDF_PAGES, len(reader.pages))}",
            multiple_tables=True,
            pandas_options={'header': None}
        )
        tables_md = "\n\n".join([df.to_markdown() for df in tables if not df.empty])
        
        return {
            "text": text[:MAX_TEXT_LENGTH],
            "tables": tables_md[:MAX_TEXT_LENGTH]
        }
    except Exception as e:
        logger.error(f"PDF processing error: {e}")
        return {"error": str(e)}
# --- Excel Processing ---
def process_excel(excel_bytes: BytesIO) -> dict:
    """Extract data from Excel files with error handling"""
    MAX_EXCEL_ROWS = 1000  # Define reasonable limit for rows to process
    
    try:
        # Read Excel file
        df_dict = pd.read_excel(excel_bytes, sheet_name=None)
        
        results = {}
        
        # Process each sheet
        for sheet_name, df in df_dict.items():
            # Limit rows for processing
            df = df.head(MAX_EXCEL_ROWS)
            
            # Basic statistics
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                stats = df[numeric_cols].describe().to_markdown()
            else:
                stats = "No numeric data found for statistics"
            
            # Convert to markdown for better display
            table_md = df.to_markdown(index=False)
            
            # Generate summary
            summary = {
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": df.columns.tolist(),
                "missing_values": df.isna().sum().to_dict(),
            }
            
            results[sheet_name] = {
                "summary": summary,
                "statistics": stats,
                "table": table_md[:MAX_TEXT_LENGTH]
            }
        
        return {
            "sheets": list(results.keys()),
            "data": results,
            "total_sheets": len(results)
        }
    except Exception as e:
        logger.error(f"Excel processing error: {e}")
        return {"error": str(e)}
# --- AI Integration ---
# Ú©Ø´ Ø¨Ø§ Ø²Ù…Ø§Ù† Ø§Ù†Ù‚Ø¶Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ AI
ai_cache = TTLCache(maxsize=CACHE_SIZE, ttl=CACHE_TTL)

@cached(cache=ai_cache)
def query_deepseek(prompt: str, use_reasoner: bool = False) -> str:
    """Get AI response with TTL caching and improved error handling"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
    model = "deepseek-chat" # Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø² Ù…Ø¯Ù„ deepseek-chat Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,
        "max_tokens": 500
    }
    
    # ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø¯Ø¯
    max_retries = 3
    
    for retry in range(max_retries):
        try:
            # Ø§ÙØ²Ø§ÛŒØ´ timeout Ø¨Ù‡ 60 Ø«Ø§Ù†ÛŒÙ‡
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            logger.error(f"API Error (attempt {retry+1}/{max_retries}): {e}")
            if retry < max_retries - 1:
                # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ø² ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯
                wait_time = 3 * (retry + 1)  # 3, 6, 9 Ø«Ø§Ù†ÛŒÙ‡
                logger.info(f"Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
            else:
                return "âš  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø±ÙˆØ± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."


    # --- Financial API Integration ---
@cached(cache=news_cache)
def get_financial_news(keywords: str = "", limit: int = 10) -> list:
    """Get financial news with TTL caching"""
    try:
        params = {
            "function": "NEWS_SENTIMENT",
            "apikey": ALPHA_VANTAGE_API_KEY,
            "limit": limit
        }
        
        if keywords:
            params["tickers"] = keywords
        
        response = requests.get(ALPHA_VANTAGE_BASE_URL, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if "feed" not in data:
            return [{"title": "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±", "url": ""}]
        
        news_items = []
        for item in data["feed"][:limit]:
            news_items.append({
                "title": item.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"),
                "summary": item.get("summary", "")[:100] + "...",
                "url": item.get("url", ""),
                "time_published": item.get("time_published", ""),
                "sentiment": item.get("overall_sentiment_label", "neutral")
            })
        
        return news_items
    except Exception as e:
        logger.error(f"Financial news API error: {e}")
        return [{"title": f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±: {str(e)}", "url": ""}]

@cached(cache=stock_cache)
def get_stock_data(symbol: str) -> dict:
    """Get stock data with TTL caching"""
    try:
        # Get company profile
        profile_url = f"{FMP_BASE_URL}/profile/{symbol}?apikey={FINANCIAL_MODELING_PREP_API_KEY}"
        profile_response = requests.get(profile_url, timeout=10)
        profile_response.raise_for_status()
        profile_data = profile_response.json()
        
        if not profile_data or len(profile_data) == 0:
            return {"error": "Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        # Get financial ratios
        ratios_url = f"{FMP_BASE_URL}/ratios/{symbol}?limit=1&apikey={FINANCIAL_MODELING_PREP_API_KEY}"
        ratios_response = requests.get(ratios_url, timeout=10)
        ratios_response.raise_for_status()
        ratios_data = ratios_response.json()
        
        # Combine data
        result = {
            "profile": profile_data[0],
            "ratios": ratios_data[0] if ratios_data else {},
        }
        
        return result
    except Exception as e:
        logger.error(f"Stock data API error: {e}")
        return {"error": str(e)}

def generate_financial_chart(data: Dict[str, Any], chart_type: str = "price") -> BytesIO:
    """Generate financial charts based on data"""
    try:
        plt.figure(figsize=(10, 6))
        
        if chart_type == "price" and "historical" in data:
            dates = [item["date"] for item in data["historical"]]
            prices = [item["close"] for item in data["historical"]]
            
            plt.plot(dates, prices)
            plt.title(f"Historical Prices: {data.get('profile', {}).get('companyName', 'Unknown')}")
            plt.xlabel("Date")
            plt.ylabel("Price")
            plt.xticks(rotation=45)
            plt.tight_layout()
        
        # Save to buffer
        buf = BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        plt.close()
        
        return buf
    except Exception as e:
        logger.error(f"Chart generation error: {e}")
        # Return a simple error image
        plt.figure(figsize=(5, 3))
        plt.text(0.5, 0.5, f"Error generating chart: {str(e)}", 
                 horizontalalignment='center', verticalalignment='center')
        plt.axis('off')
        buf = BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        plt.close()
        return buf

# --- ØªÙˆØ§Ø¨Ø¹ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø§ÛŒØ±Ø§Ù† ---
def get_iran_market_data() -> dict:
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†"""
    # Ø³Ø§Ø®Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ
    market_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "market_status": "Ø¨Ø§Ø²" if datetime.now().hour < 12 else "Ø¨Ø³ØªÙ‡",
        "overall_index": f"{1_700_000 + random.randint(-50_000, 50_000):,}",
        "market_value": f"{12_345_678:,} Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
        "trade_volume": f"{5_432:,} Ù…ÛŒÙ„ÛŒÙˆÙ† Ø³Ù‡Ù…",
        "market_trend": random.choice(["Ù…Ø«Ø¨Øª", "Ù…Ù†ÙÛŒ", "Ø®Ù†Ø«ÛŒ"]),
        "positive_symbols": random.randint(150, 300),
        "negative_symbols": random.randint(150, 300),
        "neutral_symbols": random.randint(50, 100),
    }
    
    return market_data

def get_iran_stock_data(symbol: str) -> dict:
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù‡Ø§Ù… Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†"""
    # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ
    stock_data_dict = {
        "Ø®ÙˆØ¯Ø±Ùˆ": {
            "full_name": "Ø§ÛŒØ±Ø§Ù† Ø®ÙˆØ¯Ø±Ùˆ",
            "price": "2,450",
            "change_percent": "+3.5%",
            "industry": "Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ù‚Ø·Ø¹Ø§Øª",
            "market_cap": "98,000 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": "6.8",
            "eps": "360"
        },
        "ÙÙˆÙ„Ø§Ø¯": {
            "full_name": "ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø§ØµÙÙ‡Ø§Ù†",
            "price": "3,780",
            "change_percent": "-1.2%",
            "industry": "ÙÙ„Ø²Ø§Øª Ø§Ø³Ø§Ø³ÛŒ",
            "market_cap": "283,500 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": "4.2",
            "eps": "900"
        },
                "Ø´Ø³ØªØ§": {
            "full_name": "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ ØªØ£Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ",
            "price": "4,120",
            "change_percent": "+0.8%",
            "industry": "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú†Ù†Ø¯ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ",
            "market_cap": "412,000 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": "5.3",
            "eps": "778"
        },
        "ÙˆØ¨Ù…Ù„Øª": {
            "full_name": "Ø¨Ø§Ù†Ú© Ù…Ù„Øª",
            "price": "5,230",
            "change_percent": "+2.1%",
            "industry": "Ø¨Ø§Ù†Ú©â€ŒÙ‡Ø§ Ùˆ Ù…ÙˆØ³Ø³Ø§Øª Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ",
            "market_cap": "157,000 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": "7.1",
            "eps": "736"
        },
        "ÙØ§Ø±Ø³": {
            "full_name": "ØµÙ†Ø§ÛŒØ¹ Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒ Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³",
            "price": "8,640",
            "change_percent": "-0.7%",
            "industry": "Ù…Ø­ØµÙˆÙ„Ø§Øª Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ",
            "market_cap": "518,400 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": "6.2",
            "eps": "1,393"
        }
    }
    
    # Ø§Ú¯Ø± Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù† Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if symbol in stock_data_dict:
        data = stock_data_dict[symbol]
        stock_data = {
            "symbol": symbol,
            "full_name": data["full_name"],
            "price": data["price"],
            "change_percent": data["change_percent"],
            "industry": data["industry"],
            "market_cap": data["market_cap"],
            "p/e": data["p/e"],
            "eps": data["eps"],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø³Ø§Ø²
    else:
        stock_data = {
            "symbol": symbol,
            "full_name": f"Ø´Ø±Ú©Øª {symbol}",
            "price": f"{random.randint(1000, 10000):,}",
            "change_percent": f"{random.choice(['+', '-'])}{random.uniform(0.1, 5.0):.1f}%",
            "industry": random.choice(["Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ù‚Ø·Ø¹Ø§Øª", "Ø¨Ø§Ù†Ú©â€ŒÙ‡Ø§", "ÙÙ„Ø²Ø§Øª Ø§Ø³Ø§Ø³ÛŒ", "Ù…Ø­ØµÙˆÙ„Ø§Øª Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ", "Ø³ÛŒÙ…Ø§Ù†", "Ø¯Ø§Ø±ÙˆÛŒÛŒ"]),
            "market_cap": f"{random.randint(10000, 500000):,} Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø±ÛŒØ§Ù„",
            "p/e": f"{random.uniform(3.0, 12.0):.1f}",
            "eps": f"{random.randint(100, 2000):,}",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    # Ø³Ø§Ø®Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù‚ÛŒÙ…Øª Ø³Ø§Ø®ØªÚ¯ÛŒ
    base_price = int(stock_data["price"].replace(",", "")) if "," in stock_data["price"] else random.randint(1000, 10000)
    history = []
    
    for i in range(7):  # 7 Ø±ÙˆØ² Ø§Ø®ÛŒØ±
        day = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        price_change = random.randint(-200, 200)
        price = max(100, base_price + price_change)
        
        history.append({
            "date": day,
            "close_price": f"{price:,}",
            "volume": f"{random.randint(100_000, 1_000_000):,}"
        })
    
    stock_data["history"] = history
    
    return stock_data

def get_codal_reports(symbol: str) -> list:
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„"""
    # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø¹Ø±ÙˆÙ
    company_names = {
        "Ø®ÙˆØ¯Ø±Ùˆ": "Ø§ÛŒØ±Ø§Ù† Ø®ÙˆØ¯Ø±Ùˆ",
        "ÙÙˆÙ„Ø§Ø¯": "ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø§ØµÙÙ‡Ø§Ù†",
        "Ø´Ø³ØªØ§": "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ ØªØ£Ù…ÛŒÙ† Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ",
        "ÙˆØ¨Ù…Ù„Øª": "Ø¨Ø§Ù†Ú© Ù…Ù„Øª",
        "ÙØ§Ø±Ø³": "ØµÙ†Ø§ÛŒØ¹ Ù¾ØªØ±ÙˆØ´ÛŒÙ…ÛŒ Ø®Ù„ÛŒØ¬ ÙØ§Ø±Ø³"
    }
    
    # Ø§Ú¯Ø± Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ù†Ø§Ù… Ø¢Ù† Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    company_name = company_names.get(symbol, f"Ø´Ø±Ú©Øª {symbol}")
    
    # Ø§Ù†ÙˆØ§Ø¹ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„
    report_types = [
        "ØµÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù…ÛŒØ§Ù†â€ŒØ¯ÙˆØ±Ù‡â€ŒØ§ÛŒ",
        "Ú¯Ø²Ø§Ø±Ø´ ÙØ¹Ø§Ù„ÛŒØª Ù…Ø§Ù‡Ø§Ù†Ù‡",
        "Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡",
        "Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø±Ø¢Ù…Ø¯",
        "ØªØµÙ…ÛŒÙ…Ø§Øª Ù…Ø¬Ù…Ø¹",
        "Ø§ÙØ´Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø§Ù‡Ù…ÛŒØª"
    ]
    
    reports = []
    for i in range(5):  # 5 Ú¯Ø²Ø§Ø±Ø´ Ø§Ø®ÛŒØ±
        # ØªØ§Ø±ÛŒØ® ØªØµØ§Ø¯ÙÛŒ Ø¯Ø± 3 Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±
        days_ago = random.randint(0, 90)
        report_date = (datetime.now() - timedelta(days=days_ago)).strftime("%Y-%m-%d")
        
        # Ù†ÙˆØ¹ Ú¯Ø²Ø§Ø±Ø´ ØªØµØ§Ø¯ÙÛŒ
        report_type = report_types[i % len(report_types)]
        
        reports.append({
            "date": report_date,
            "title": f"Ú¯Ø²Ø§Ø±Ø´ {report_type} Ø´Ø±Ú©Øª {company_name}",
            "category": report_type,
            "url": f"https://www.codal.ir/Reports/Report{i+1}.aspx"
        })
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
    reports.sort(key=lambda x: x["date"], reverse=True)
    
    return {
        "symbol": symbol,
        "company_name": company_name,
        "reports": reports
    }

# --- Telegram Handlers ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Initiate conversation with knowledge level selection"""
    keyboard = [
        [InlineKeyboardButton("Ù…Ø¨ØªØ¯ÛŒ", callback_data="level_beginner"),
         InlineKeyboardButton("Ù…ØªÙˆØ³Ø·", callback_data="level_intermediate"),
         InlineKeyboardButton("Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ", callback_data="level_pro")]
    ]
    await update.message.reply_text(
        "ğŸ¯ Ù„Ø·ÙØ§Ù‹ Ø³Ø·Ø­ Ø¯Ø§Ù†Ø´ Ù…Ø§Ù„ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
        reply_markup=InlineKeyboardMarkup(keyboard)
    )

async def set_knowledge_level(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle knowledge level selection"""
    query = update.callback_query
    await query.answer()
    
    level = query.data.split("_")[1]
    context.user_data["knowledge_level"] = level
    
    # ØªØºÛŒÛŒØ± ÙØ±Ù…Øª Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù†
    level_names = {
        "beginner": "Ù…Ø¨ØªØ¯ÛŒ",
        "intermediate": "Ù…ØªÙˆØ³Ø·",
        "pro": "Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ"
    }
    
    message_text = f"âœ… Ø³Ø·Ø­ Ø¯Ø§Ù†Ø´ Ø´Ù…Ø§ Ø¨Ù‡ {level_names.get(level, level)} ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯!\n\n"
    message_text += "Ø§Ú©Ù†ÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯ ÛŒØ§ ÙØ§ÛŒÙ„ Excel ÛŒØ§ PDF ØµÙˆØ±Øª Ù…Ø§Ù„ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.\n\n"
    message_text += "Ø¯Ø³ØªÙˆØ±Ø§Øª Ø§ØµÙ„ÛŒ:\n"
    message_text += "/news [Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ] - Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ù…Ø§Ù„ÛŒ\n"
    message_text += "/stock [Ù†Ù…Ø§Ø¯] - Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù…\n"
    message_text += "/market - Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±\n"
    message_text += "/help - Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„\n\n"
    message_text += "Ø¯Ø³ØªÙˆØ±Ø§Øª Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†:\n"
    message_text += "â€¢ /iran_market - Ù…Ø´Ø§Ù‡Ø¯Ù‡ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø±\n"
    message_text += "â€¢ /iran_stock [Ù†Ù…Ø§Ø¯] - ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù† (Ù…Ø«Ø§Ù„: /iran_stock Ø®ÙˆØ¯Ø±Ùˆ)\n"
    message_text += "â€¢ /codal [Ù†Ù…Ø§Ø¯] - Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„"
    
    await query.edit_message_text(message_text)


async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Process text queries with level-appropriate responses"""
    user_input = update.message.text
    user_id = update.effective_user.id
    
    # Get user's knowledge level
    level = context.user_data.get("knowledge_level", "beginner")
    
    # Create level-specific prompt
    prompt_templates = {
        "beginner": "Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ùˆ Ø¨Ø§ Ù…Ø«Ø§Ù„ Ù…Ù„Ù…ÙˆØ³ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯:",
        "intermediate": "Ø¨Ø§ Ø°Ú©Ø± Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ØªØ®ØµØµÛŒ ÙˆÙ„ÛŒ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯:",
        "pro": "Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ†ÛŒ Ú©Ø§Ù…Ù„ Ùˆ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:"
    }
    
    base_prompt = f"""
    Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ø§Ù„ÛŒ Ø³Ø·Ø­ {level} Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:
    Ø³ÙˆØ§Ù„: {user_input}
    {prompt_templates[level]}
    """
    
    # Normalize for caching
    clean_prompt = normalize_prompt(base_prompt)
    
    # ØªØ¹ÛŒÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Reasoner Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø·Ø­ Ø¯Ø§Ù†Ø´ Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ Ù…Ø­ØªÙˆØ§ÛŒ Ø³ÙˆØ§Ù„
    use_reasoner = False
    if level == "pro":
        use_reasoner = True
    elif any(keyword in user_input.lower() for keyword in ["ØªØ­Ù„ÛŒÙ„ Ù…Ø§Ù„ÛŒ", "Ù†Ø³Ø¨Øª Ù…Ø§Ù„ÛŒ", "ØµÙˆØ±Øª Ù…Ø§Ù„ÛŒ", "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ"]):
        use_reasoner = True
    
    # Get cached or new response with appropriate model
    response = query_deepseek(clean_prompt, use_reasoner=use_reasoner)
    
    # Send response and request feedback
    await update.message.reply_text(
        f"ğŸ“Š Ù¾Ø§Ø³Ø® ØªØ­Ù„ÛŒÙ„Ú¯Ø±:\n\n{response}",
        reply_markup=InlineKeyboardMarkup([[
            InlineKeyboardButton("ğŸ‘ Ù…ÙÛŒØ¯ Ø¨ÙˆØ¯", callback_data="feedback_good"),
            InlineKeyboardButton("ğŸ‘ Ù…ÙÛŒØ¯ Ù†Ø¨ÙˆØ¯", callback_data="feedback_bad")
        ]])
    )


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Process PDF and Excel financial documents"""
    document = update.message.document
    file_name = document.file_name.lower()
    
    # Check file type
    if file_name.endswith('.pdf'):
        try:
            # Download and process PDF
            await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ PDF...")
            file = await context.bot.get_file(document.file_id)
            pdf_bytes = BytesIO(await file.download_as_bytearray())
            processed = process_pdf(pdf_bytes)
            
            if "error" in processed:
                raise Exception(processed["error"])
            
            # Create analysis prompt
            prompt = f"""
            ØªØ­Ù„ÛŒÙ„ ØµÙˆØ±Øª Ù…Ø§Ù„ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:
            Ù…ØªÙ† Ø§ØµÙ„ÛŒ: {processed['text']}
            Ø¬Ø¯Ø§ÙˆÙ„: {processed['tables']}
            
            Ù…ÙˆØ§Ø±Ø¯ ØªØ­Ù„ÛŒÙ„:
            1- Ù†Ù‚Ø§Ø· Ù‚ÙˆØª/Ø¶Ø¹Ù Ù…Ø§Ù„ÛŒ
            2- Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
            3- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
            """
            
            # Get and send response
            response = query_deepseek(normalize_prompt(prompt), use_reasoner=True)
            await update.message.reply_text(f"ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ ØµÙˆØ±Øª Ù…Ø§Ù„ÛŒ:\n\n{response}")
            
        except Exception as e:
            logger.error(f"PDF Error: {e}")
            await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ù†Ø¯ PDF: {str(e)}")
    
    elif file_name.endswith(('.xls', '.xlsx', '.xlsm')):
        await handle_excel(update, context)
    
    else:
        await update.message.reply_text("âŒ ÙØ±Ù…Øª ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ PDF ÛŒØ§ Excel Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.")

async def handle_excel(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Process Excel financial data"""
    if not update.message.document.file_name.endswith(('.xls', '.xlsx', '.xlsm')):
        await update.message.reply_text("âŒ Ù„Ø·ÙØ§Ù‹ ÙÙ‚Ø· ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.")
        return
    
    try:
        # Download and process Excel
        await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„...")
        file = await context.bot.get_file(update.message.document.file_id)
        excel_bytes = BytesIO(await file.download_as_bytearray())
        processed = process_excel(excel_bytes)
        
        if "error" in processed:
            raise Exception(processed["error"])
        
        # Create analysis prompt
        sheets_info = "\n".join([f"- {sheet}" for sheet in processed["sheets"]])
        first_sheet = processed["sheets"][0]
        first_sheet_data = processed["data"][first_sheet]
        
        prompt = f"""
        ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø§Ú©Ø³Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:
        
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„:
        - ØªØ¹Ø¯Ø§Ø¯ Ø´ÛŒØªâ€ŒÙ‡Ø§: {processed["total_sheets"]}
        - Ø´ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {sheets_info}
        
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´ÛŒØª Ø§ÙˆÙ„ ({first_sheet}):
        - ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ø±Ù‡Ø§: {first_sheet_data["summary"]["rows"]}
        - ØªØ¹Ø¯Ø§Ø¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {first_sheet_data["summary"]["columns"]}
        - Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {", ".join(first_sheet_data["summary"]["column_names"])}
        
        Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ:
        {first_sheet_data["statistics"]}
        
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:
        {first_sheet_data["table"]}
        
        Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ØŒ Ø´Ø§Ù…Ù„:
        1- Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
        2- Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù…Ù‡Ù…
        3- ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
        4- Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ ÛŒØ§ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
        """
        
        # Get and send response
        response = query_deepseek(normalize_prompt(prompt), use_reasoner=True)
        await update.message.reply_text(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„:\n\n{response}")
        
    except Exception as e:
        logger.error(f"Excel Error: {e}")
        await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„: {str(e)}")

async def handle_feedback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Collect user feedback"""
    query = update.callback_query
    await query.answer()
    
    feedback_type = query.data.split("_")[1]
    logger.info(f"Feedback received: {feedback_type}")
    
    # Store feedback (can be extended to database)
    with open("feedback.log", "a") as f:
        f.write(f"{time.time()},{feedback_type}\n")
    
    await query.edit_message_text("ğŸ™ Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ø´Ù…Ø§ Ù…ØªØ´Ú©Ø±ÛŒÙ…!")

async def get_news(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Fetch and send financial news"""
    keywords = " ".join(context.args) if context.args else ""
    
    await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù…Ø§Ù„ÛŒ...")
    
    news = get_financial_news(keywords)
    
    if not news or "error" in news[0]["title"]:
        await update.message.reply_text("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±. Ù„Ø·ÙØ§Ù‹ Ø¨Ø¹Ø¯Ø§Ù‹ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
        return
    
    # Format news without markdown
    news_text = "ğŸ“° Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ù…Ø§Ù„ÛŒ\n\n"
    for item in news:
        sentiment_emoji = "ğŸ˜"
        if item["sentiment"] == "positive":
            sentiment_emoji = "ğŸŸ¢"
        elif item["sentiment"] == "negative":
            sentiment_emoji = "ğŸ”´"
            
        news_text += f"{item['title']} {sentiment_emoji}\n"
        news_text += f"{item['summary']}\n"
        news_text += f"Ù„ÛŒÙ†Ú© Ø®Ø¨Ø±: {item['url']}\n\n"
    
    await update.message.reply_text(news_text, disable_web_page_preview=True)


async def get_stock_info(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Fetch and analyze stock information"""
    if not context.args:
        await update.message.reply_text("âŒ Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ø³Ù‡Ø§Ù… Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯. Ù…Ø«Ø§Ù„: /stock AAPL")
        return
    
    symbol = context.args[0].upper()
    await update.message.reply_text(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù‡Ø§Ù… {symbol}...")
    
    stock_data = get_stock_data(symbol)
    
    if "error" in stock_data:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø§: {stock_data['error']}")
        return
    
    # Format stock information
    profile = stock_data["profile"]
    ratios = stock_data["ratios"]
    
    info_text = f"ğŸ“ˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù‡Ø§Ù… {symbol}\n\n"
    info_text += f"{profile.get('companyName', 'N/A')}\n"
    info_text += f"Ù‚ÛŒÙ…Øª: ${profile.get('price', 'N/A')}\n"
    info_text += f"ØªØºÛŒÛŒØ±: {profile.get('changes', 'N/A')} ({profile.get('changesPercentage', 'N/A')}%)\n"
    info_text += f"ØµÙ†Ø¹Øª: {profile.get('industry', 'N/A')}\n\n"
    
    info_text += "Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ:\n"
    if ratios:
        info_text += f"P/E: {ratios.get('priceEarningsRatio', 'N/A')}\n"
        info_text += f"P/B: {ratios.get('priceToBookRatio', 'N/A')}\n"
        info_text += f"ROE: {ratios.get('returnOnEquity', 'N/A')}\n"
        info_text += f"ROA: {ratios.get('returnOnAssets', 'N/A')}\n"
        info_text += f"Debt to Equity: {ratios.get('debtToEquity', 'N/A')}\n"
    else:
        info_text += "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.\n"
    
    # Add analysis using AI
    analysis_prompt = f"""
    ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:
    
    Ù†Ø§Ù… Ø´Ø±Ú©Øª: {profile.get('companyName', 'N/A')}
    Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: ${profile.get('price', 'N/A')}
    ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª: {profile.get('changes', 'N/A')} ({profile.get('changesPercentage', 'N/A')}%)
    ØµÙ†Ø¹Øª: {profile.get('industry', 'N/A')}
    ØªÙˆØ¶ÛŒØ­Ø§Øª: {profile.get('description', 'N/A')}
    
    Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ:
    P/E: {ratios.get('priceEarningsRatio', 'N/A')}
    P/B: {ratios.get('priceToBookRatio', 'N/A')}
    ROE: {ratios.get('returnOnEquity', 'N/A')}
    ROA: {ratios.get('returnOnAssets', 'N/A')}
    Debt to Equity: {ratios.get('debtToEquity', 'N/A')}
    
    Ù„Ø·ÙØ§Ù‹ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø§ÛŒÙ† Ø³Ù‡Ø§Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.
    """
    
    analysis = query_deepseek(normalize_prompt(analysis_prompt), use_reasoner=True)
    info_text += f"\nØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯:\n{analysis}"
    
    await update.message.reply_text(info_text)


async def market_summary(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Provide a summary of current market conditions"""
    await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ ØªÙ‡ÛŒÙ‡ Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§Ø²Ø§Ø±...")
    
    try:
        # Get major indices data
        indices = ["^GSPC", "^DJI", "^IXIC", "^FTSE", "^N225"]
        indices_names = {
            "^GSPC": "S&P 500",
            "^DJI": "Dow Jones",
            "^IXIC": "Nasdaq",
            "^FTSE": "FTSE 100",
            "^N225": "Nikkei 225"
        }
        
        indices_data = {}
        for idx in indices:
            data = get_stock_data(idx)
            if "error" not in data:
                indices_data[indices_names.get(idx, idx)] = data
        
        # Get top gainers and losers
        gainers_url = f"{FMP_BASE_URL}/stock_market/gainers?apikey={FINANCIAL_MODELING_PREP_API_KEY}"
        gainers_response = requests.get(gainers_url, timeout=10)
        gainers_data = gainers_response.json()[:5]  # Top 5 gainers
        
        losers_url = f"{FMP_BASE_URL}/stock_market/losers?apikey={FINANCIAL_MODELING_PREP_API_KEY}"
        losers_response = requests.get(losers_url, timeout=10)
        losers_data = losers_response.json()[:5]  # Top 5 losers
        
        # Format market summary
        summary_text = "ğŸŒ Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±\n\n"
        
        # Add indices
        summary_text += "Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ:\n"
        for name, data in indices_data.items():
            profile = data.get("profile", {})
            summary_text += f"{name}: ${profile.get('price', 'N/A')} ({profile.get('changesPercentage', 'N/A')}%)\n"
        
        # Add gainers
        summary_text += "\nØ¨ÛŒØ´ØªØ±ÛŒÙ† Ø±Ø´Ø¯:\n"
        for item in gainers_data:
            summary_text += f"{item.get('symbol', 'N/A')} ({item.get('companyName', 'N/A')}): "
            summary_text += f"${item.get('price', 'N/A')} ({item.get('changesPercentage', 'N/A')}%)\n"
        
        # Add losers
        summary_text += "\nØ¨ÛŒØ´ØªØ±ÛŒÙ† Ø§ÙØª:\n"
        for item in losers_data:
            summary_text += f"{item.get('symbol', 'N/A')} ({item.get('companyName', 'N/A')}): "
            summary_text += f"${item.get('price', 'N/A')} ({item.get('changesPercentage', 'N/A')}%)\n"
        
        # Get AI analysis of market conditions
        market_prompt = f"""
        Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ±ØŒ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:
        
        Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ:
        {', '.join([f"{name}: {data.get('profile', {}).get('changesPercentage', 'N/A')}%" for name, data in indices_data.items()])}
        
        Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø±Ø´Ø¯:
        {', '.join([f"{item.get('symbol', 'N/A')}: {item.get('changesPercentage', 'N/A')}%" for item in gainers_data])}
        
        Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§ÙØª:
        {', '.join([f"{item.get('symbol', 'N/A')}: {item.get('changesPercentage', 'N/A')}%" for item in losers_data])}
        
        Ù„Ø·ÙØ§Ù‹ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø§Ø² Ø±ÙˆÙ†Ø¯ Ø¨Ø§Ø²Ø§Ø±ØŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Ùˆ Ø¶Ø¹ÛŒÙØŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        """
        
        market_analysis = query_deepseek(normalize_prompt(market_prompt), use_reasoner=True)
        summary_text += f"\nØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø±:\n{market_analysis}"
        
        await update.message.reply_text(summary_text)
        
    except Exception as e:
        logger.error(f"Market summary error: {e}")
        await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§Ø²Ø§Ø±: {str(e)}")




async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Display help information"""
    help_text = "ğŸ¤– Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø±Ø¨Ø§Øª ØªØ­Ù„ÛŒÙ„ Ù…Ø§Ù„ÛŒ\n\n"
    help_text += "Ø¯Ø³ØªÙˆØ±Ø§Øª Ø§ØµÙ„ÛŒ:\n"
    help_text += "/start - Ø´Ø±ÙˆØ¹ Ú©Ø§Ø± Ø¨Ø§ Ø±Ø¨Ø§Øª Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø³Ø·Ø­ Ø¯Ø§Ù†Ø´\n"
    help_text += "/help - Ù†Ù…Ø§ÛŒØ´ Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§\n"
    help_text += "/news [Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ] - Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ù…Ø§Ù„ÛŒ\n"
    help_text += "/stock [Ù†Ù…Ø§Ø¯] - Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù…\n"
    help_text += "/market - Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±\n\n"
    help_text += "Ø¯Ø³ØªÙˆØ±Ø§Øª Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†:\n"
    help_text += "/iran_market - ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†\n"
    help_text += "/iran_stock [Ù†Ù…Ø§Ø¯] - ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù† (Ù…Ø«Ø§Ù„: /iran_stock Ø®ÙˆØ¯Ø±Ùˆ)\n"
    help_text += "/codal [Ù†Ù…Ø§Ø¯] - Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„ ÛŒÚ© Ø´Ø±Ú©Øª (Ù…Ø«Ø§Ù„: /codal Ø®ÙˆØ¯Ø±Ùˆ)\n\n"
    help_text += "Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø±Ø¨Ø§Øª:\n"
    help_text += "â€¢ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø§Ù„ÛŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø³Ø·Ø­ Ø¯Ø§Ù†Ø´ Ø´Ù…Ø§\n"
    help_text += "â€¢ ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ØµÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ\n"
    help_text += "â€¢ ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Excel Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ\n"
    help_text += "â€¢ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù…Ø§Ù„ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ\n"
    help_text += "â€¢ ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ùˆ Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ\n"
    help_text += "â€¢ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†\n"
    help_text += "â€¢ ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†\n"
    help_text += "â€¢ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„\n\n"
    help_text += "Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„ØŒ Ú©Ø§ÙÛŒØ³Øª ÙØ§ÛŒÙ„ PDF ÛŒØ§ Excel Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.\n"
    help_text += "Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø§Ù„ÛŒØŒ Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯."
    
    await update.message.reply_text(help_text)


async def iran_market(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†"""
    try:
        await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø§Ø²Ø§Ø±
        market_data = get_iran_market_data()
        
        response = f"ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†\n\n"
        response += f"ğŸ•’ Ø²Ù…Ø§Ù†: {market_data['timestamp']}\n"
        response += f"ğŸ› ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±: {market_data['market_status']}\n"
        response += f"ğŸ“ˆ Ø´Ø§Ø®Øµ Ú©Ù„: {market_data['overall_index']}\n"
        response += f"ğŸ’° Ø§Ø±Ø²Ø´ Ø¨Ø§Ø²Ø§Ø±: {market_data['market_value']}\n"
        response += f"ğŸ“Š Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {market_data['trade_volume']}\n"
        response += f"ğŸ”„ Ø±ÙˆÙ†Ø¯ Ú©Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø±: {market_data['market_trend']}\n\n"
        response += f"ÙˆØ¶Ø¹ÛŒØª Ù†Ù…Ø§Ø¯Ù‡Ø§:\n"
        response += f"ğŸŸ¢ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª: {market_data['positive_symbols']}\n"
        response += f"ğŸ”´ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ: {market_data['negative_symbols']}\n"
        response += f"âšª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±: {market_data['neutral_symbols']}\n"
        
        await update.message.reply_text(response)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± iran_market: {e}")
        await update.message.reply_text("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²Ø§Ø±. Ù„Ø·ÙØ§Ù‹ Ø¨Ø¹Ø¯Ø§Ù‹ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")

async def iran_stock(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù†"""
    try:
        if not context.args:
            await update.message.reply_text("âŒ Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ø³Ù‡Ø§Ù… Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯. Ù…Ø«Ø§Ù„: /iran_stock Ø®ÙˆØ¯Ø±Ùˆ")
            return
        
        symbol = context.args[0]
        await update.message.reply_text(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù‡Ø§Ù… {symbol}...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø³Ù‡Ø§Ù…
        stock_data = get_iran_stock_data(symbol)
        
        # Ø³Ø§Ø®Øª Ù…ØªÙ† ØªØ§Ø±ÛŒØ®Ú†Ù‡
        history_text = "\nğŸ“… ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù‚ÛŒÙ…Øª (7 Ø±ÙˆØ² Ø§Ø®ÛŒØ±):\n"
        for item in stock_data["history"]:
            history_text += f"- {item['date']}: {item['close_price']} (Ø­Ø¬Ù…: {item['volume']})\n"
        
        response = f"ğŸ” Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù‡Ø§Ù… {stock_data['symbol']}\n\n"
        response += f"ğŸ“ Ù†Ø§Ù… Ú©Ø§Ù…Ù„: {stock_data['full_name']}\n"
        response += f"ğŸ’° Ù‚ÛŒÙ…Øª: {stock_data['price']} Ø±ÛŒØ§Ù„\n"
        response += f"ğŸ“Š ØªØºÛŒÛŒØ±Ø§Øª: {stock_data['change_percent']}\n"
        response += f"ğŸ­ ØµÙ†Ø¹Øª: {stock_data['industry']}\n"
        response += f"ğŸ’¼ Ø§Ø±Ø²Ø´ Ø¨Ø§Ø²Ø§Ø±: {stock_data['market_cap']}\n"
        response += f"ğŸ“ˆ Ù†Ø³Ø¨Øª P/E: {stock_data['p/e']}\n"
        response += f"ğŸ’µ EPS: {stock_data['eps']} Ø±ÛŒØ§Ù„\n"
        response += f"ğŸ•’ Ø²Ù…Ø§Ù†: {stock_data['timestamp']}\n"
        response += history_text
        
        # ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AI
        analysis_prompt = f"""
        ØªØ­Ù„ÛŒÙ„ Ø³Ù‡Ø§Ù… Ø²ÛŒØ± Ø§Ø² Ø¨Ø§Ø²Ø§Ø± Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù† Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:
        
        Ù†Ø§Ù… Ø´Ø±Ú©Øª: {stock_data['full_name']}
        Ù†Ù…Ø§Ø¯: {stock_data['symbol']}
        Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: {stock_data['price']} Ø±ÛŒØ§Ù„
        ØªØºÛŒÛŒØ±Ø§Øª: {stock_data['change_percent']}
        ØµÙ†Ø¹Øª: {stock_data['industry']}
        Ø§Ø±Ø²Ø´ Ø¨Ø§Ø²Ø§Ø±: {stock_data['market_cap']}
        Ù†Ø³Ø¨Øª P/E: {stock_data['p/e']}
        EPS: {stock_data['eps']} Ø±ÛŒØ§Ù„
        
        ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù‚ÛŒÙ…Øª (7 Ø±ÙˆØ² Ø§Ø®ÛŒØ±):
        {', '.join([f"{item['date']}: {item['close_price']}" for item in stock_data["history"]])}
        
        Ù„Ø·ÙØ§Ù‹ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ø§ÛŒÙ† Ø³Ù‡Ø§Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.
        """
        
        analysis = query_deepseek(normalize_prompt(analysis_prompt), use_reasoner=True)
        response += f"\nØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯:\n{analysis}"
        
        await update.message.reply_text(response)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± iran_stock: {e}")
        await update.message.reply_text("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù‡Ø§Ù…. Ù„Ø·ÙØ§Ù‹ Ø¨Ø¹Ø¯Ø§Ù‹ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")


async def codal_reports_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø´Ø±Ú©Øª"""
    try:
        if not context.args:
            await update.message.reply_text("âŒ Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ø´Ø±Ú©Øª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯. Ù…Ø«Ø§Ù„: /codal Ø®ÙˆØ¯Ø±Ùˆ")
            return
        
        symbol = context.args[0]
        await update.message.reply_text(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„ Ø¨Ø±Ø§ÛŒ {symbol}...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªÚ¯ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„
        codal_data = get_codal_reports(symbol)
        
        response = f"ğŸ“‘ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„ Ø¨Ø±Ø§ÛŒ {codal_data['company_name']} ({codal_data['symbol']})\n\n"
        response += f"ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(codal_data['reports'])}\n\n"
        response += "Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±:\n"
        
        for i, report in enumerate(codal_data['reports'], 1):
            response += f"{i}. {report['date']} - {report['title']} ({report['category']})\n"
            response += f"   Ù„ÛŒÙ†Ú© Ú¯Ø²Ø§Ø±Ø´: {report['url']}\n"
        
        await update.message.reply_text(response)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± codal: {e}")
        await update.message.reply_text("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø¯Ø§Ù„. Ù„Ø·ÙØ§Ù‹ Ø¨Ø¹Ø¯Ø§Ù‹ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")


# --- Main Execution ---
def run_bot():
    """Configure and run the bot"""
    try:
        print(f"Attempting to create bot with token: {TELEGRAM_TOKEN[:5]}...")
        
        # ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒÙ¾â€ŒØ³ÛŒÚ©
        if not test_deepseek_connection():
            print("âš  Ù‡Ø´Ø¯Ø§Ø±: Ø§ØªØµØ§Ù„ Ø¨Ù‡ API Ø¯ÛŒÙ¾â€ŒØ³ÛŒÚ© Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ø±Ø¨Ø§Øª Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        
        application = Application.builder().token(TELEGRAM_TOKEN).build()
        
        # Add handlers
        application.add_handler(CommandHandler("start", start))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("news", get_news))
        application.add_handler(CommandHandler("stock", get_stock_info))
        application.add_handler(CommandHandler("market", market_summary))
        
        # Ø¯Ø³ØªÙˆØ±Ø§Øª Ø¨ÙˆØ±Ø³ Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ù‡Ø± Ø¯Ùˆ ÙØ±Ù…Øª
        application.add_handler(CommandHandler("iran_market", iran_market))
        application.add_handler(CommandHandler("iranmarket", iran_market))  # Ø¨Ø¯ÙˆÙ† Ø¢Ù†Ø¯Ø±Ù„Ø§ÛŒÙ†
        application.add_handler(CommandHandler("iran_stock", iran_stock))
        application.add_handler(CommandHandler("iranstock", iran_stock))  # Ø¨Ø¯ÙˆÙ† Ø¢Ù†Ø¯Ø±Ù„Ø§ÛŒÙ†
        
        # Ø¯Ø³ØªÙˆØ±Ø§Øª Ú©Ø¯Ø§Ù„
        application.add_handler(CommandHandler("codal", codal_reports_command))
        
        # Ø³Ø§ÛŒØ± Ù‡Ù†Ø¯Ù„Ø±Ù‡Ø§
        application.add_handler(CallbackQueryHandler(set_knowledge_level, pattern="^level_"))
        application.add_handler(CallbackQueryHandler(handle_feedback, pattern="^feedback_"))
        application.add_handler(MessageHandler(filters.Document.ALL, handle_document))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
        
        # ØªÙˆÙ‚Ù Ù…Ø·Ù…Ø¦Ù†
        def signal_handler(sig, frame):
            print("\nØ¯Ø±ÛŒØ§ÙØª Ø³ÛŒÚ¯Ù†Ø§Ù„ ØªÙˆÙ‚Ù. Ø¯Ø± Ø­Ø§Ù„ Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡...")
            application.stop()
            sys.exit(0)        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        print("Ø±Ø¨Ø§Øª Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ ØªÙˆÙ‚ÙØŒ Ctrl+C Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        application.run_polling()
    except Exception as e:
        print(f"Error starting bot: {e}")
        sys.exit(1)

if __name__ == "__main__":    run_bot()
 